{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a262f72-d82a-4d32-908c-260efad16018",
   "metadata": {},
   "source": [
    "# ESBOÇO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a609db2-f639-4942-9e6c-00e8ce42cb4f",
   "metadata": {},
   "source": [
    "Acredito que a melhor estratégia seria a de dividir o nosso objetivo em objetivos menores, e esses objetivos menores devem ser repartidos novamente, na intenção de simplificar ao máximo nossa tarefa e assim torna-la mais viável e fácil de se entender cada parte do processo que estamos tentando criar.\n",
    "\n",
    "Nosso objetivo é automatizar a avaliação de redações escritas ao estilo do ENEM contruindo uma CAR deve ser capaz de: \n",
    "1.  Verificar se há desvios no texto\n",
    "2.  Atribuir uma nota a redação (seja ela global ou dividida em critérios)\n",
    "3.  Retornar um feedback ao aluno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab516f58-3559-47c2-9e00-3f31c1e20019",
   "metadata": {},
   "source": [
    "## 0 - Pré-processamento de Dados, Engenharia de Features e Análise Exploratória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc6e3c-5d32-43b4-9a40-a12a81080bdb",
   "metadata": {},
   "source": [
    "Partindo do princípio que haverá um banco de dados ou algo semelhante que possa ser usado para se treinar um modelo de aprendizado de máquina, a primeira coisa a se fazer é sempre tratar esses dados, realizar qualquer tratamento necessario para que seja possível retirar o máximo proveito das informações ali contidas. \n",
    "\n",
    "Além disso pode ser que seja necessária a criação de novas colunas dentro dessa tabela, semelhante ao trabalho que fiz no conjunto público de redações que me foi apontado (Essay-br). Essas colunas viriam a conter informações como a quantidade de palavras dentro de cada redação, um vetor com a quantidade de vírgulas dentro de cada frase, a quantiadde de eros ortográficos, as palavras mais frequêntes, entidades mencionadas, como locais, pessoas ou personagens públicos, possíveis citações ou palavras escritas em outro idioma e assim por diante.\n",
    "\n",
    "Aqui seriam feitos os processos de tokenização e lematização, entre outros, que posteriormente permitiriam a um  modelo de machine learning ter um bem desempenho. Seria feita também a avaliação de normalidade dos dados numéricos e, se for o caso, sua normalização.\n",
    "\n",
    "Depois talvez fosse necessária uma análise exploratória a fim de se verificar possíveis padrões e correlações dentro desses dados, informações que podem vir a ser úteis nas próximas etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0455e-da95-45f8-bab6-905349417129",
   "metadata": {},
   "source": [
    "## 1 - Verificar se há desvios no Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfdbc8-5e56-4405-9f12-dd338a526f03",
   "metadata": {},
   "source": [
    "Levando em conta que a maior parte dos candidatos que fazem o ENEM e sua redação são brasileiros, ou seja, possuem o português como sua língua nativa, sugiro que a essa parte do processo seja feita utilizando a chamada abordagem simbólica baseada em regras, pois parece ser a mais adequeda nesse contexto, sem contar que a mesma permite mostrar o erro ao candidato, se for o caso, permitindo-o entender o que está errado e o que pode ser melhorado.\n",
    "Isso nos ajudará posteriormente na construção do feedback para o candidato.\n",
    "\n",
    "A depender da situação poderíamos incluir também alguma abordagem estatística, mas somente se necessario, pois a mesma não é tão clara quanto a primeira e não nos permite dar um feedback ao candidato de forma tão simples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a0e7c9-6e01-462d-b12b-e242cb3d97e2",
   "metadata": {},
   "source": [
    "Para a identificação desses desvios alguns pacotes bem comuns de Python poderiams ser utilizados, como spaCy com funções como \"token.pos_\" , o language_tool_python com funções como \"LanguageTool.check(texto)\" e \"LanguageTool.correct(texto)\" e o TextBlob com funções como TextBlob.correct().\n",
    "\n",
    "Importante observar que a verificação de desvios é também uma etapa da avaliação das competências requeridas na redação do ENEM, portanto, um processo que verifica um caso acaba verificando o outro. Como nesse caso foi dada muita ênfase na verificação de desvios no texto, proponho que essa verificação seja feita antes do processo de Atribuição de Nota e que essas informações sejam reaproveitadas posteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a63d96-dee9-40a1-beaa-f74a820a35f0",
   "metadata": {},
   "source": [
    "## 2 - Atribuição de Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b98ef-826b-451b-84ed-c836afa758ff",
   "metadata": {},
   "source": [
    "Como mencionei anteriormente, acredito que separar um determinado objetivo em etapas menores é a melhor forma de se realiza-lo, de modo que a atribuição de nota por critério me parece mais fácil e adequada.\n",
    "\n",
    "Após os dados terem passado pela etapa de pré-processamento, engenharia de features e etc, o próximo passo seria o de separa-lo em 3 grupos: treino, validação e teste, com uma proporção que gire em torno de 70%, 15% e 15% respectivamente. Os três grupos são necessários para a criação de um algoritmo avançado, um para treinar o modelo em si, outro para ajustar os parâmetros e o último para finalmente testar os resultados. \n",
    "\n",
    "Nesse caso eu estratificaria os grupos com base em uma variável chamada \"Zero?\" que indica se a redação está zerada ou não, pois acredito que uma redação com nota total zero e outras com nota total diferentes de zero podem conter características diferentes, de modo que me parece importante que as amostras fiquem balanceadas enquanto a isso.\n",
    "\n",
    "O código que eu usaria seria algo mais ou menos como o código a seguir: (código meramente ilustrativo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe04bbf-8128-44be-983c-d16b61622b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import pandas as pd\n",
    "\n",
    "# X = data[['X1', 'X2', 'X3']] # DEMAIS VARIÀVEIS\n",
    "# y = data['Zero?']            # VARIÀVEL QUE INDICA SE A REDAÇÃO ESTÁ ZERADA OU NÃO\n",
    "\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(X,      y,      test_size=0.3, stratify=y,      random_state=42)\n",
    "# X_val,   X_test, y_val,   y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d6f8e-e671-4c06-866b-b32a35e7b293",
   "metadata": {},
   "source": [
    "### Caso de dataset pequeno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19725189-0550-4a4e-b5d8-5f1bdd975923",
   "metadata": {},
   "source": [
    "Caso o dataset disponível para o treinamento do modelo seja pequeno, eu escolheria trabalhar utlizando o modelo de Random Forest combinado a técnicas como Bag-of-Words, pois são relativamente simples, são os que mais se adequam a situações com uma quantidade de dados não muito grande e são fáceis de se trabalhar e de se evitar o overfitting. (Sem contar que são modelos com os quais já tenho experiência em campo.)\n",
    "\n",
    "Ainda assim, deria difícil criar um algoritmo realmente satisfatório dada a complexidade de nosso objetivo, alguns objetivos menores poderiam ser alcançados por esse meio, mas dificilmente seria possivel satifazer totalmente nossas expectativas com uma pequena quantidade de dados. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e7bc5-d272-4d56-a4b0-670a1dff62c2",
   "metadata": {},
   "source": [
    "### Caso de dataset grande"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881aff8-9c2f-4742-ad75-eb928d375d72",
   "metadata": {},
   "source": [
    "Idealmente falando, acredito que um dos melhores modelos de aprendizado de máquina para esses casos é o de redes neurais profundas. Nesse caso em específico eu sugiro uma combinação desse modelo com word embeddings. Para esclarecer, word embeddings são representações de palavras dentro de um espaço contínuo, de modo que o espaço ocupado por essa palavra é definido por suas relações semânticas e contextuais com outras palavras, ou seja, palavras com significados semelhantes estão próximas umas das outras. \n",
    "\n",
    "As duas opções de arquitetura mais viáveis para esse caso seriam a arquitetura LSTM (Long Short-Term Memory) ou a GRU (Gated Recurrent Unit), a primeira tem demonstrado bons resultados em Processamento de Linguagem Natural, mas é um pouco complexa e pode requerer muito poder computacional, a segunda é mais simples, mas pode não ser tão poderosa quanto a primeira, pois é feita para quando se tem recursos mais limitados. Qual das duas escolher dependeria da situação e dos resultados de alguns testes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6304259b-d02f-4838-803c-168496ac12ff",
   "metadata": {},
   "source": [
    "Vale lembrar que uma arquitetura de rede neural é apenas à maneira como os diferentes componentes de uma rede neural são organizados e conectados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a202b2-ff4f-4b83-9c4b-3ea08e9883d3",
   "metadata": {},
   "source": [
    "### Competências na Redação do ENEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fd906-e137-449f-bc00-1d6f3ae0530b",
   "metadata": {},
   "source": [
    "A avaliação das competências a seguir não seria feita necessariamente da mesma forma, ou com os mesmos algoritmos, técnicas e ferramentas, já que algumas avaliações buscam verificar diferentes características e possuem diferentes graus de complecidade, onde algumas são bastante complexas e outras nem tanto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88626730-5cbd-4a76-b355-ee5968c6431e",
   "metadata": {},
   "source": [
    "#### Língua Portuguesa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e3f78d-1554-49b4-8613-769a1297fd31",
   "metadata": {},
   "source": [
    "Esse é, talvez a competência mais fácil de se ser avaliada de modo sistemático. \n",
    "\n",
    "É possível verificar a possível existência de erros ortográficos com o pacote pyspellchecker, verificar estruturas gramaticais com o pacote spaCy e de pontuação e outros erros com o pacote language_tool_python. Caso o candidato venha a demonstrar total domínio da linguagem formal portuguesa, ele tiraria 200 (nota máxima)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8e5cc-8569-4332-969c-aa0a2b59579f",
   "metadata": {},
   "source": [
    "#### Gênero e Tema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3a1a62-5e75-4dd5-826d-83558d273fe3",
   "metadata": {},
   "source": [
    "A identificação de repertório poderia nos ajudar a avaliar essa competência, por exemplo, ao se identificar entidades mencionadas na redação, como ffoi feito na análise exploratória feita anteriormente, o candidato demonstra ter repertório sobre a situação-problema e que entende como o mesmo se relaciona a outros assuntos (caso a entidade mencionada tenha alguma relação com o tema proposto, é claro). \n",
    "\n",
    "A verificação do gênero ficaria a cargo do modelo de aprendizado de máquina treinado com a partir da base de dados fornecida, já que é uma questão mais difícil de se verificar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb68a53-f8d7-415d-bd9d-dc217ce5c979",
   "metadata": {},
   "source": [
    "#### Coerência"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef56b2-522f-41f5-b4b8-3299fee1af01",
   "metadata": {},
   "source": [
    "Aqui, em se tratando de uma competência muito subjetiva, é onde mais será necessário o uso de um açgoritmo de redes neurais profundas junto a word embeddings, pois dificilmente um algoritmo menos sofisticado será capaz de realizar tal avaliação de forma satisfatória. Ainda assim, acredito ser necessário a supervisão de pelo menos um agente humano para verificar o trabalho realizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387feaf3-39d2-4329-8f41-38215fb16609",
   "metadata": {},
   "source": [
    "#### Coesão"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170f5a5-e811-42b2-a9bd-96400dc8bd7b",
   "metadata": {},
   "source": [
    "Pode-se usar algumas métricas já estabelidas para verificar a coesão do texto, por exemplo, a coesão lexical pode ser verificada pelo repetição de palavras e uso de pronomes. A coesão sintática pode ser medida usando a distância entre sujeitos e predicados e também pelo uso de conjunções e conectivos. A coesão semântica pode ser verificada pela similaridade semântica entre as palavras. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a42afc-2146-4807-bf53-a7e6d590bde0",
   "metadata": {},
   "source": [
    "#### Proposta de Intervenção"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38e2f1-ada1-4dc4-8e79-5a21b6921ac1",
   "metadata": {},
   "source": [
    "Essa é, possivelmente, a competência mais difícil de se ser verificada de forma sistemática. Mesmo com o uso de redes neurais profundas e outras técnicas avançadas seira difícl alcançar um resultado satisfatório nesse quesito. A identificação de entidades mencionadas pode ajudar nessa questão, mas mesmo assim não parece ser possível sistematizar de forma realmente eficiente tal avaliação a não ser que possamos contar com o auxílio de alguma ferramenta como o CHAT-GPT ou assistente semelhante. Acredito que nesse caso um agente humano seja indispensável para uma avaliação realmente satisfatória."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef72c3-5ca7-4aa1-a79c-8df3d6a85715",
   "metadata": {},
   "source": [
    "Naturalmente, a nota global seria, assim como no ENEM, a soma das notas das cinco conpetências citadas acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e909892-9d7c-4e96-b299-51f3c4f171e3",
   "metadata": {},
   "source": [
    "## 3 - Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c035b1-3d6e-409d-9b28-09c656726abd",
   "metadata": {},
   "source": [
    "Para começar o feedback deveria apontar para métricas básicas do texto feito pelo candidato, como a quantidade de conectivos, erros de gramática e ortografia, a quantidade de palavras e pontuação inadequadas e etc. A fim de reproduzir um trabalho similar ao feito no NILC-Metrix, utilizando o Python,  podemos utilizar os pacotes nltk, spaCy e transformers, entre outros, para entregar essas estatísticas básicas para o candidato. Uma outra forma se de apontar pontos fortes, ou fracos, na redação seria a presença de entidades citadas dentro da texto que estivessem de alguma forma relacionadas ao tema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f738b0-21fe-4b56-8f40-11487f051fb9",
   "metadata": {},
   "source": [
    "Ainda assim essas informações, embora sejam um começo, não seriam de muita utilizade para o aluno que queira se sair melhor em uma próxima redação. Tenho motivos para acreditar que, sem o auxílio de ferramentas como um assistente virtual ou de um agente humano envolvido, não é possível entregar um feedback satisfatório, um feedback mais pedagógico, de modo que o candidato consiga ter uma melhor percepção de seus pontos fortes e fracos e buscar melhorar sua performance em uma possível próxima redação. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c53d5-cda1-44fd-9795-590034ee6b84",
   "metadata": {},
   "source": [
    "Resumindo, a maior dificuldade não é dar um feedback em si, mas sim dar ao candidato um feedback mais pedagógico, que o ajude a entender os motivos de sua nota, seja demonstrando seus pontos fortes ou fracos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb1941-32d1-448b-aa57-c5bd5a998175",
   "metadata": {},
   "source": [
    "## Caso de não haver uma Base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2ac23-5723-4713-9cfd-c000ae76c41c",
   "metadata": {},
   "source": [
    "No caso de não haver uma base de dados que nos permita treinar um modelo de aprendizado de máquina, toda a construção da Correção Automática de Redações teria que ser feita por regras, ainda assim algumas delas podem ser encontradas em pacotes da linguagem Python, enquanto que outras teriam que ser criadas por nós mesmos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268707b-93ba-44d1-b991-9405eb6d523f",
   "metadata": {},
   "source": [
    "## Observações Finais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1e5bb-690b-41b8-abec-6cab48e849ae",
   "metadata": {},
   "source": [
    "Independentemente de quão bom seja o resultado de todo o trabalho na construção de um sistema de Correção Automática de Redação, acredito que, ao menos por hora, o sistema de CAR mais eficiente será resultado da combinação de uma correção automática com uma manual, ou seja, de um agente humano trabalhando junto a uma algoritmo, de modo que um possa conpensar ou corrigir qualquer viés ou erro do outro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271ef02-3f2a-465f-96f2-d1f6eeaea8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
