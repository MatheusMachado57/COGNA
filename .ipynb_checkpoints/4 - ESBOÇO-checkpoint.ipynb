{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a262f72-d82a-4d32-908c-260efad16018",
   "metadata": {},
   "source": [
    "# ESBOÇO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a609db2-f639-4942-9e6c-00e8ce42cb4f",
   "metadata": {},
   "source": [
    "Acredito que a melhor estratégia seria a de dividir o nosso objetivo em objetivos menores, e esses objetivos menores devem ser repartidos novamente, na intenção de simplificar ao máximo nossa tarefa e assim torna-la mais viável e fácil de se entender cada parte do processo que estamos tentando criar.\n",
    "\n",
    "Nosso objetivo é automatizar a avaliação de redações escritas ao estilo do ENEM contruindo uma CAR deve ser capaz de: \n",
    "1.  Verificar se há desvios no texto\n",
    "2.  Atribuir uma nota a redação (seja ela global ou dividida em critérios)\n",
    "3.  Retornar um feedback ao aluno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab516f58-3559-47c2-9e00-3f31c1e20019",
   "metadata": {},
   "source": [
    "## 0 - Pré-processamento de Dados, Engenharia de Features e Análise Exploratória"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcc6e3c-5d32-43b4-9a40-a12a81080bdb",
   "metadata": {},
   "source": [
    "A primeira etapa a se fazer com qualquer projeto que envolva um banco de dados que ainda não foi trabalhado é uma análise exploratória, supondo-se que seja apenas uma tabela, por exemplo, deve-se verificar quantas linhas e colunas essa tabela possui, qual informação cada coluna possui, quais variáveis são numéricas e quais são categóricas, quantas linhas e quais colunas tem informação faltando e etc. Em seguida devemos limpar esses dados, realizar qualquer tratamento necessário para que seja possível retirar o máximo proveito das informações ali contidas. \n",
    "\n",
    "Além disso, pode ser que seja necessária a criação de novas colunas dentro dessa tabela, semelhante ao trabalho que fiz no conjunto público de redações que me foi apontado (Essay-br). Essas colunas viriam a conter informações como a quantidade de palavras dentro de cada redação, um vetor com a quantidade de vírgulas dentro de cada frase, a quantiadde de erros ortográficos, as palavras mais frequêntes, entidades mencionadas, como locais, pessoas ou personagens públicos, possíveis citações ou palavras escritas em outro idioma e assim por diante.\n",
    "\n",
    "Nessa etapa inicial também seriam feitos os processos de tokenização e lematização, entre outros, que posteriormente permitiriam a um  modelo de aprendizado de máquina ter um bom desempenho. Seria feita também a avaliação de normalidade dos dados numéricos e, se for o caso, sua normalização. Ao final dessa etapa os dados devem estar devidamente explorados e trabalhados, prontos para o uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df0455e-da95-45f8-bab6-905349417129",
   "metadata": {},
   "source": [
    "## 1 - Verificar se há desvios no Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cfdbc8-5e56-4405-9f12-dd338a526f03",
   "metadata": {},
   "source": [
    "Levando em conta que a maior parte dos candidatos que fazem o ENEM e sua redação são brasileiros, ou seja, possuem o português como sua língua nativa, sugiro que a essa parte do processo seja feita utilizando a chamada abordagem simbólica baseada em regras, pois, apesar de obsoleta sob certos aspectos, parece ser a mais adequeda nesse contexto. Além disso essa abordagem permite mostrar o erro comitido ao candidato permitindo-o entender o que está errado e o que pode ser melhorado, isso nos ajudará posteriormente na construção do feedback para o candidato.\n",
    "\n",
    "A depender da situação poderíamos incluir também alguma abordagem estatística, mas somente se necessario, pois a mesma não é tão clara quanto a primeira e não nos permite dar ao candidato um feedback tão claro.\n",
    "\n",
    "Para identificar desvios nos textos existem alguns pacotes de Python que podem ser utilizados para esse fim.\n",
    "\n",
    ". Para desvios ortográficos temos: SpellChecker() e unknown() do pacote pyspellchecker, spellcheck() do pacote textblob entre outros.\n",
    "\n",
    ". Para desvios gramaticais temos: LanguageToolPublicAPI() e check() do pacote language_tool_python, e o pacote nltk que permite a criação de regras gramaticais simples.\n",
    "\n",
    ". Para desvios de vocabulário, ergistro e gênero: spacy.load() e nlp() do pacote spaCY, e modelos pré treinados como Word2Vec.\n",
    "\n",
    ". Para desvios no uso de recursos coesivos: para esses casos podemos usar algumas das funções, pacotes e modelos já citados.\n",
    "\n",
    "Há ainda outros tipos de desvios, que no geral podem ser identificados de forma semelhante aos já mencionados.\n",
    "\n",
    "Importante observar que os resultados dessa primeira etapa afetam diretamente as outras duas próximas etapas, portanto, esses resultados devem ser usados posteriormente, tanto no momento de atribuir nota as redações quanto no momento de se gerar um feedback para o aluno. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a63d96-dee9-40a1-beaa-f74a820a35f0",
   "metadata": {},
   "source": [
    "## 2 - Atribuição de Nota"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158b98ef-826b-451b-84ed-c836afa758ff",
   "metadata": {},
   "source": [
    "Como mencionei anteriormente, acredito que separar um determinado objetivo em etapas menores é a melhor forma de se realiza-lo, de modo que a atribuição de nota por critério me parece mais fácil e adequada. Nesse caso, como os dados que me foram indicados são de redações do ENEM, os critérios serão os mesmos do exeme em questão.\n",
    "\n",
    "Após os dados terem passado pela etapa de pré-processamento, engenharia de features e etc, o próximo passo seria o de separa-lo em 3 grupos: treino, validação e teste, com uma proporção que gire em torno de 70%, 15% e 15%, respectivamente. Os três grupos são necessários para a criação de um algoritmo avançado, um para treinar o modelo em si, outro para ajustar os parâmetros e o último para finalmente testar os resultados. \n",
    "\n",
    "Nesse caso eu estratificaria os grupos com base em uma variável chamada \"Zero?\" e outra chamada \"Mil?\",  a primeira indicaria se a redação está zerada ou não, e a segunda se a redação possui nota mil ou não, pois acredito que uma redação com nota total zero ou mil podem conter características diferentes das outras redações, de modo que me parece importante que as amostras fiquem balanceadas enquanto a isso. Lembrando que essa mesma estratégia pde vir a ser aplicada em outras variáveis que venham a se mostrar importante sob esses mesmo prisma.\n",
    "\n",
    "Usando uma técnica de aprendizado supervisionado por regressão, no final do processo teremos um modelo de aprendizado de máquina que nos permitirá, com maior ou menor precisão, gerar uma nota para cada critério de redação e, por fim, uma nota global."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398d6f8e-e671-4c06-866b-b32a35e7b293",
   "metadata": {},
   "source": [
    "### Caso de termos um pequeno Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19725189-0550-4a4e-b5d8-5f1bdd975923",
   "metadata": {},
   "source": [
    "Caso o dataset disponível para o treinamento do modelo seja pequeno, eu escolheria trabalhar utlizando o modelo de Random Forest combinado a técnicas como Bag-of-Words e Emsemble, pois são relativamente simples, são os que mais se adequam a situações com uma quantidade de dados não muito grande e são fáceis de se trabalhar e de se evitar o overfitting. (Sem contar que são modelos com os quais já tenho experiência em campo.)\n",
    "\n",
    "Ainda assim, deria difícil criar um algoritmo realmente satisfatório dada a complexidade de nosso objetivo, alguns objetivos menores poderiam ser alcançados por esse meio, mas dificilmente seria possivel satifazer totalmente nossas expectativas com uma pequena quantidade de dados. \n",
    "\n",
    "Em Python, a função RandomForestRegressor do pacote sklearn.ensemble pode ser usada para esse fim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038e7bc5-d272-4d56-a4b0-670a1dff62c2",
   "metadata": {},
   "source": [
    "### Caso de termos um grande DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881aff8-9c2f-4742-ad75-eb928d375d72",
   "metadata": {},
   "source": [
    "Idealmente falando, acredito que um dos melhores modelos de aprendizado de máquina para esses casos é o de redes neurais profundas. Nesse caso em específico eu sugiro uma combinação desse modelo com word embeddings. Para esclarecer, word embeddings são representações de palavras dentro de um espaço contínuo, de modo que o espaço ocupado por essa palavra é definido por suas relações semânticas e contextuais com outras palavras, ou seja, palavras com significados semelhantes estão próximas umas das outras. \n",
    "\n",
    "As duas opções de arquitetura mais viáveis para esse caso seriam a arquitetura LSTM (Long Short-Term Memory) ou a GRU (Gated Recurrent Unit), a primeira tem demonstrado bons resultados em Processamento de Linguagem Natural, mas é um pouco complexa e pode requerer muito poder computacional, a segunda é mais simples, mas pode não ser tão poderosa quanto a primeira, pois é feita para quando se tem recursos mais limitados. Qual das duas escolher dependeria da situação e dos resultados de alguns testes.\n",
    "\n",
    "O pacote usado seria o TensorFlow, e as funções seriam Sequential e dentro dessa função as funções Embedding, LSTM ou GRU (a depender da arquitetura escolhida). O pacote Pythorch também poderia ser uma opção. \n",
    "\n",
    "Vale lembrar que uma arquitetura de rede neural é apenas à maneira como os diferentes componentes de uma rede neural são organizados e conectados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a202b2-ff4f-4b83-9c4b-3ea08e9883d3",
   "metadata": {},
   "source": [
    "### As 4 Primeiras Competências na Redação do ENEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52fd906-e137-449f-bc00-1d6f3ae0530b",
   "metadata": {},
   "source": [
    "As quatro primeiras competências poderiam ser avaliadas usando funções, métodos e features que já foram citados até então, como por exemplo os resultados da Etapa 1 poderiam auxiliar na primeira competência, Língua Portuguesa, e o modelo de aprendizado de máquina poderia ser usado na atribuição das notas de Coerência e Coesão, que são noções mais abstratas e, portanto, mais difícieis de se verificar. \n",
    "\n",
    "Já parte da segunda competência, Tema, pode ser melhor avaliado por atributos dependentes de conteúdo, como exemplo as matrizes de termos e métricas calculadas a partir dessas matrizes, como a similaridade de cosseno utilizada entre tema e redação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38e2f1-ada1-4dc4-8e79-5a21b6921ac1",
   "metadata": {},
   "source": [
    "### Proposta de Intervenção\n",
    "\n",
    "Essa é, possivelmente, a competência mais difícil de se ser verificada de forma sistemática. Mesmo com o uso de redes neurais profundas e outras técnicas avançadas seira difícl alcançar um resultado satisfatório nesse quesito. A identificação de entidades mencionadas pode ajudar nessa questão, mas mesmo assim não parece ser possível sistematizar de forma realmente eficiente tal avaliação a não ser que possamos contar com o auxílio de alguma ferramenta como o CHAT-GPT ou assistente semelhante. Acredito que nesse caso um agente humano seja indispensável para uma avaliação realmente satisfatória."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef72c3-5ca7-4aa1-a79c-8df3d6a85715",
   "metadata": {},
   "source": [
    "Naturalmente, a nota global seria, assim como no ENEM, a soma das notas das cinco conpetências citadas acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e909892-9d7c-4e96-b299-51f3c4f171e3",
   "metadata": {},
   "source": [
    "## 3 - Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c035b1-3d6e-409d-9b28-09c656726abd",
   "metadata": {},
   "source": [
    "Para começar, o feedback poderia apontar métricas básicas do texto feito pelo candidato, como a quantidade de conectivos, erros de gramática e ortografia, a quantidade de palavras e pontuação inadequadas, diversidade lexical, conectivos e etc. A fim de facilitar o trabalho, podemos buscar fazer um trabalho similar ao feito no NILC-Metrix (ferramente criada para realizar análises linguísticas em português), utilizando informações e números obtidos nas primeiras duas etapas para obtermos essas métricas básicas. Uma outra forma se de apontar pontos fortes ou fracos na redação seria a presença ou ausência de entidades citadas dentro da texto que estivessem de alguma forma relacionadas ao tema.\n",
    "\n",
    "Ainda assim essas informações, embora sejam um começo, não seriam de muita utilizade para o aluno que queira entender seus pontos fortes e fracos para se sair melhor em uma próxima redação. Tenho motivos para acreditar que, sem o auxílio de ferramentas como um assistente virtual ou de um agente humano envolvido, seria muito difícil entregar um feedback satisfatório, algo mais pedagógico, de modo que o candidato consiga ter uma melhor percepção do que pode ser melhorado em uma possível próxima redação. \n",
    "\n",
    "Resumindo, a maior dificuldade não seria dar ao candidato um feedback em si, mas sim dar um feedback mais pedagógico, que o ajude a entender os motivos de sua nota, seja demonstrando seus pontos fortes ou fracos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbb1941-32d1-448b-aa57-c5bd5a998175",
   "metadata": {},
   "source": [
    "## Caso de não haver uma Base de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be2ac23-5723-4713-9cfd-c000ae76c41c",
   "metadata": {},
   "source": [
    "No caso de não haver uma base de dados que nos permita treinar um modelo de aprendizado de máquina, toda a construção da Correção Automática de Redações teria que ser feita por regras, ainda assim algumas delas podem ser encontradas em pacotes da linguagem Python, enquanto que outras teriam que ser criadas por nós mesmos. O resultado final certariamente deixaria muito a desejar, mas é o que seria possível de se fazer em tais circunstâncias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2268707b-93ba-44d1-b991-9405eb6d523f",
   "metadata": {},
   "source": [
    "## Observações Finais"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd1e5bb-690b-41b8-abec-6cab48e849ae",
   "metadata": {},
   "source": [
    "Independentemente de quão bom seja o sistema de Correção Automática de Redação construído, acredito que, ao menos por hora, o sistema de CAR mais eficiente será resultado da combinação de uma correção automática com uma manual, ou seja, de um agente humano trabalhando junto a uma algoritmo, de modo que um possa conpensar ou corrigir qualquer viés ou erro do outro."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
